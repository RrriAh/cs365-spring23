{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "\n",
    "## Exact Gradient Computation\n",
    "\n",
    "Given a function f, sometimes we can compute its exact gradient at any x if f's derivative is easy to compute. For example, let $f(x)=2x^2-3x+\\ln x$, where $x>0$. Please compute the derivative of f and report its gradient at $x=2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: \n",
    "\n",
    "f'(x) = 4x-3+1/x\n",
    "\n",
    "f'(2) = 8-3+1/2 = 11/2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient Computation [5 pts]\n",
    "\n",
    "Instead of computing the derivative of a function, we can also estimate the gradient numerically with various methods. These methods are essential, especially when a callable function is not exposed due to privacy reasons, or it is hard to differentiate analytically. \n",
    "\n",
    "To numerically compute the gradient, the simple way is to follow Newton's difference quotient: $f'(x)=\\lim_{h\\to 0}{f(x+h)-f(x)\\over h}$. Another two-point formula is to compute the slope through the points $(x-h,f(x-h))$ and $(x+h,f(x+h))$. Let us reuse the example function $f(x)=2x^2-3x+\\ln x$ and test the precision of these two approaches. Define the function in the next cell, and try to compute its gradient via both methods at $x=2$. Range h value in [0.1,0.01,0.001,0.0001] and report all gradients calculated. Which method is more accurate, and why does it work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "l = [0.1,0.01,0.001,0.0001]\n",
    "\n",
    "def f(x):\n",
    "    # Your code here\n",
    "    return (2*x**2-3*x+math.log(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.687901641694317, 5.518754151103744, 5.50187504165045, 5.5001875004201395]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient using the first method (Newton's difference quotient)\n",
    "def method1 (f,x,h):\n",
    "    return (f(x+h)-f(x))/h\n",
    "\n",
    "\n",
    "g = []\n",
    "for h in l:\n",
    "    g.append(method1(f,2,h))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.500417292784904, 5.500004166729123, 5.500000041666448, 5.500000000412448]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient using the second method \n",
    "def method2 (f,x,h):\n",
    "    s_y = f(x+h)-f(x-h)\n",
    "    s_x = (x+h) - (x-h)\n",
    "    slope = s_y/s_x\n",
    "    return slope\n",
    "g = []\n",
    "for h in l:\n",
    "    g.append(method2(f,2,h))\n",
    "print(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: You may find the gradient more accurate when using a smaller h value. However, this is not always the case. Due to the finite precision of the floating-point, rounding errors always exist and can dominate the computation when the h value is too small. Run the following two cells to observe such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.551115123125783\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-15\n",
    "print((f(2+eps)-f(2-eps))/2/eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-20\n",
    "print((f(2+eps)-f(2-eps))/2/eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a classification tool that models the probability of an event taking place by having the log odds for the event be a linear combination of one or more independent variables. Specifically, let $\\vec{x}=<x_1,\\dots ,x_m>$ be an m dimensional vector of independent variables (features), and $y$ be the corresponding binary dependent variable (label). The probability of having $y=1$ is modeled as $$P_y={1\\over 1+e^{-(b_0+b_1\\cdot x_1+\\dots +b_m\\cdot x_m)}}={1\\over 1+e^{-(b_0+\\vec{b}_{1:m}\\cdot\\vec{x})}}$$\n",
    "\n",
    "Given a set of data points $<\\vec{x}_k,y_k>$ with $k\\in [1,n]$, how can we fit the model with these data, i.e., how to choose the best $\\vec{b}=b_0,b_1\\cdots,b_m$?\n",
    "\n",
    "One way is to write out the likelihood $$\\prod_{k:y_k=1}P_{y_k}\\prod_{k:y_k=0}(1-P_{y_k})$$ and find $b_0,b_1\\cdots,b_m$ that maximize its logarithm, $$l=\\sum_{k:y_k=1}\\ln(P_{y_k})+\\sum_{k:y_k=0}\\ln(1-P_{y_k})$$\n",
    "\n",
    "In contrast to computing the closed form gradient of a Least-squares loss in a linear model (chapter 5 of MML book), doing the same for logistic regression is not possible. Gradient descent can be used to optimize such function $l$, and we will implement it step-by-step. First, write a function log_likelihood in the next cell that computes the log-likelihood given data points and $\\vec{b}$. [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X,y,b):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    b: one dimension numpy data array of length m+1\n",
    "    \n",
    "    Return the log likelihood.\n",
    "    \"\"\"\n",
    "    y_1 = 0\n",
    "    y_2 = 0\n",
    "    for i in range(len(X)):\n",
    "        u = b[0]+b[1:]@X[i][:]\n",
    "        p_y = 1/(1+ np.exp(-u))\n",
    "        if y[i] == 1:\n",
    "            y_1+= math.log(p_y)  \n",
    "        else:\n",
    "            y_2 += math.log(1-p_y)\n",
    "    l = y_1+y_2\n",
    "    return l\n",
    "\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your log_likelihood function with a small example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.031735331771901"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0.1],[0.5],[1.]])\n",
    "y=np.array([0,0,1])\n",
    "b=np.array([0.,1.])\n",
    "# Your answer should be around -2.03\n",
    "log_likelihood(X,y,b)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to maximize, the next step is to compute the gradient of the log-likelihood with respect to parameter $\\vec{b}$. Use the method with Newton's difference quotient, and set $h=0.0001$. Implement the function compute_gradient in the next cell. [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,b):\n",
    "# The inputs are the same as the ones of log_likelihood\n",
    "    h = 0.0001\n",
    "    grad = []\n",
    "    for i in range(len(b)):\n",
    "        b_h = np.copy(b)\n",
    "        b_h[i]+=h\n",
    "        grad.append((log_likelihood(X,y,b_h)-log_likelihood(X,y,b))/h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.8785311466219525, -0.09479905564102609]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function here, preserve the output\n",
    "compute_gradient(X,y,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to compute the gradients, we can optimize the objective, which is log-likelihood in our case, using gradient descent. It iteratively changes the parameters in a small \"step\" towards the gradient direction, i.e., the direction where the objective increases at the fastest pace. Formally, denote the calculated gradients as $\\Delta (\\vec{b})$, we can update our parameters via $\\vec{b}=\\vec{b}+\\gamma \\cdot \\Delta (\\vec{b})$, where $\\gamma$ is the size of the \"step\". Repeat this process until the objective stop improving or a pre-set max number of iterations is reached. **Note in practice, the value of gradient changes over iterations and can be very large/small, so you should normalize the gradient vector every iteration, i.e., scale it to $\\Delta (\\vec{b})\\over ||\\Delta (\\vec{b})||_2$, before using it to compute the new $\\vec{b}$. Therefore, the update rule for parameters becomes $\\vec{b}=\\vec{b}+\\gamma \\cdot {\\Delta (\\vec{b})\\over ||\\Delta (\\vec{b})||_2}$**.\n",
    "\n",
    "Implement the gradient_descent function below. [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, initial_b, step_size, max_iteration):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    initial_b: one dimension numpy data array of length m+1\n",
    "    step_size: scalar, the size of one step update\n",
    "    max_iteration: scalar, the max number of iterations\n",
    "    Return the updated coefficient vector b.\n",
    "    \"\"\"\n",
    "    b = np.copy(initial_b)\n",
    "    for k in range(max_iteration):\n",
    "        b_g = compute_gradient(X,y,b) #gradient\n",
    "        b_n = b_g/np.linalg.norm(b_g)\n",
    "        b_previous = np.array(b)\n",
    "        b+= step_size*b_n\n",
    "        if log_likelihood(X,y,b) < log_likelihood(X,y,b_previous):\n",
    "            b = b_previous\n",
    "            break\n",
    "        \n",
    "    return k,b\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with the previous example again. Print for each sample from X, based on your model, the probability of having label=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of having label=1 for sample 0: 0.000000000000000000000021831003\n",
      "Probability of having label=1 for sample 1: 0.000000002072568169002954875897\n",
      "Probability of having label=1 for sample 2: 0.999999998371839282640394230839\n"
     ]
    }
   ],
   "source": [
    "k,optimized_b = gradient_descent(X, y, b, 0.1, 1000)\n",
    "\n",
    "# compute and print the probability for each row in X below using optimized_b\n",
    "p = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "        u = optimized_b[0]+optimized_b[1:]@X[i][:]\n",
    "        p.append(1/(1+ np.exp(-u)))\n",
    "for i, prob in enumerate(p):\n",
    " print(f\"Probability of having label=1 for sample {i}: {prob:.30f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the implemented logistic regression model to a real dataset. The dataset is a trimmed breast-cancer-Wisconsin dataset from UCI machine learning Repository. Only 100 data points are offered in the training set to make sure the computation can be finished swiftly, no matter how you implement the optimizer. The training dataset is loaded in the next cell, and the vector $\\vec{b}$ is also randomly initialized. \n",
    "\n",
    "Fit three models with the training set using different step size ranging in [0.01,0.05,0.1] and set the max number of iterations as 10000. How do the final log-likelihood value and the number of iterations change with different step sizes? [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"breast-cancer-wisconsin.data\",\"r\")\n",
    "X_train = []\n",
    "y_train = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_train.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_train.append(tmp)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "random_b = np.random.uniform(0,1,size=(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood:[-7.485570166015979, -20.108245768517058, -45.79924229738929]\n",
      "iteration:[6578, 260, 34]\n",
      "coefficient b:[array([-1.49474447e+01,  9.48452874e-01, -1.41658660e+00,  9.29689553e-01,\n",
      "        9.95545390e-01,  4.67617047e-01, -4.36983986e-04,  8.66828153e-01,\n",
      "        1.05299023e+00,  1.44164397e+00]), array([-2.92886087,  0.1966536 ,  0.24068184,  0.14110111,  0.14737802,\n",
      "        0.0249783 ,  0.17537148, -0.42920884,  0.4092927 ,  0.09522708]), array([ 0.02521586, -0.17804215,  0.58560065,  0.00327295, -0.04870264,\n",
      "       -0.19807859,  0.16778882, -0.48738013,  0.28408043,  0.21789842])]\n"
     ]
    }
   ],
   "source": [
    "# Fit three models with different step size, report the final log-likelihood, \n",
    "# number of iterations and the final coefficent vector b.\n",
    "step_list = [0.01,0.05,0.1]\n",
    "max_iter = 10000\n",
    "l = []\n",
    "b_list = []\n",
    "k_list = []\n",
    "for step in step_list:\n",
    "    k,opt_b = gradient_descent(X_train,y_train,np.array(random_b),step,max_iter)\n",
    "    k_list.append(k)\n",
    "    b_list.append(opt_b)\n",
    "    l.append(log_likelihood(X_train,y_train,opt_b))\n",
    "print(\"Log likelihood:{}\".format(l))\n",
    "print(\"iteration:{}\".format(k_list))\n",
    "print(\"coefficient b:{}\".format(b_list))    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load the test dataset, and predict for each sample in the test set what labels it should have using the model obtained. Compare your results with the ground truth labels, and report the accuracy rate. [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_data.txt\",\"r\")\n",
    "X_test = []\n",
    "y_test = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_test.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_test.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (Step size: 0.01)\n",
      "Accuracy: 30.00%\n",
      "\n",
      "Model 2 (Step size: 0.05)\n",
      "Accuracy: 60.00%\n",
      "\n",
      "Model 3 (Step size: 0.1)\n",
      "Accuracy: 80.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    " # Predict the probabilities of y=1 for the test dataset\n",
    "X_test = np.array(X_test)\n",
    "n_test = len(X_test)\n",
    "X_n = np.hstack((np.ones((n_test, 1)), X_test))\n",
    "\n",
    "p = []\n",
    "b = b_list\n",
    "for i in range(len(b_list)):\n",
    "        z = np.dot(b[i],X_n)\n",
    "        probabilities_test = 1 / (1 + np.exp(-z))\n",
    "        # Convert probabilities to binary labels using a threshold of 0.5\n",
    "        predicted_labels = (probabilities_test > 0.5).astype(int)\n",
    "        # Calculate the accuracy by comparing the predicted labels with the ground truth labels\n",
    "        y_test = np.array(y_test)\n",
    "        accuracy = np.mean(predicted_labels == y_test)*100\n",
    " \n",
    "        print(f\"Model {i+1} (Step size: {step_list[i]})\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
